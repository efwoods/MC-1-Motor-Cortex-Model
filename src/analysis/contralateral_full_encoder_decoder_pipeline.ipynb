{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c160db19",
   "metadata": {},
   "source": [
    "## Imports, Settings, & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Scipy\n",
    "from scipy.signal import butter, filtfilt, iirnotch, hilbert\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.io import savemat \n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0610e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Filters\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = fs / 2.0\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=1.0, highcut=200.0, fs=1000.0, order=4):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# Apply after bandpass\n",
    "def notch_filter(data, freq=60.0, fs=1000.0, quality=30.0):\n",
    "    b, a = iirnotch(freq, quality, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# Noise Metrics for evaluation\n",
    "def compute_rmse(true, estimate):\n",
    "    return np.sqrt(np.mean((true - estimate) ** 2))\n",
    "\n",
    "# Kurtosis signal reduction > 0 shows a denoised signal\n",
    "def proportion_of_positive_kurtosis_signals(kurtosis_raw, kurtosis_denoised):\n",
    "    return (np.array([(kurtosis_raw - kurtosis_denoised) > 0]).sum() / len(kurtosis_raw)) * 100\n",
    "\n",
    "# Use a Standard scaler to reduce the mean to 0 and std to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the power envelope of each channel\n",
    "\n",
    "def band_power_envelope(ecog_signal: np.ndarray, lowcut: float, highcut: float, fs: float = 1000.0, order: int = 4) -> np.ndarray:\n",
    "    \"\"\"Computes band-limited envelope via Hilbert transform.\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.ecog_signal : np.ndarray (T, channels)\n",
    "        This is the ecog signal that has been filtered.\n",
    "    lowcut : float\n",
    "        This is the lower band limit in Hz.\n",
    "    highcut : float\n",
    "        This is the upper band limit in Hz.\n",
    "    fs : float, optional\n",
    "        This is the frequency of the sample., by default 1000.0\n",
    "    order : int, optional\n",
    "        This is the Butterworth order, by default 4\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        envelope\n",
    "    \"\"\"\n",
    "    # 1. Narrowband bandpass\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    narrow = filtfilt(b, a, ecog_signal, axis=0)\n",
    "    # 2. Hilbert transform to get analytic signal\n",
    "    analytic = hilbert(narrow, axis=0)\n",
    "    # 3. Envelope = absolute value\n",
    "    envelope = np.abs(analytic)\n",
    "    return envelope\n",
    "\n",
    "def multiband_features(ecog_raw: np.ndarray, fs: float = 1000.0) -> np.ndarray:\n",
    "    \"\"\"Builds concatenated band-power features for μ, β, and high-gamma using a Hilbert transform.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ecog_raw : np.ndarray\n",
    "        (T, 64)\n",
    "    fs : float, optional\n",
    "        Frequency of the sample, by default 1000.0\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        features: (T, 64, 3) (μ, β, high-gamma per electrode)\n",
    "    \"\"\"\n",
    "    mu_env = band_power_envelope(ecog_raw, lowcut=8.0, highcut=13.0, fs=fs)\n",
    "    beta_env = band_power_envelope(ecog_raw, lowcut=13.0, highcut=30.0, fs=fs)\n",
    "    hg_env = band_power_envelope(ecog_raw, lowcut=70.0, highcut=200.0, fs=fs)\n",
    "    # Concatenate along channel dimension\n",
    "    return np.concatenate([mu_env, beta_env, hg_env], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlapping_windows(ecog_values: np.ndarray, motion_values: np.ndarray, window_size: int = 20, hop_size: int = 10):\n",
    "    \"\"\"Builds overlapping windows to increase sample count and capture smoother transitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ecog_values : np.ndarray\n",
    "        (T, features)\n",
    "    motion_values : np.ndarray\n",
    "        (T_motion, 3)_\n",
    "    window_size : int, optional\n",
    "        number of timepoints per window, by default 20\n",
    "    hop_size : int, optional\n",
    "        step bewteen windows, by default 10\n",
    "    \"\"\"\n",
    "    num_samples, num_features = ecog_values.shape\n",
    "    print(f\"number of Samples\")\n",
    "    max_windows = (num_samples - window_size) // hop_size + 1\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    for w in range(max_windows):\n",
    "        start = w * hop_size\n",
    "        end = start + window_size\n",
    "        if end > num_samples:\n",
    "            break\n",
    "        # Assign label as motion at center of window (or last timepoint)\n",
    "        X_list.append(ecog_values[start:end, :])\n",
    "        y_list.append(motion_values[min(end -1, motion_values.shape[0] -1), :])\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.stack(y_list, axis=0)\n",
    "    return X, y        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Preprocessing for the raw data\n",
    "class PreprocessData:\n",
    "    def __init__(self, ecog_file_path, motion_file_path):\n",
    "        self.ecog_file_path = ecog_file_path\n",
    "        self.motion_file_path = motion_file_path\n",
    "        self.ecog_data = None\n",
    "        self.motion_data = None\n",
    "        self.filtered_ecog = None\n",
    "        self.scaled_ecog = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def process(self, eval=False, window_size=20, duration_limit=900, function_timer=False, ):\n",
    "        if function_timer: preprocessing_start_time = time.perf_counter_ns()\n",
    "        if function_timer: reading_time_start = time.perf_counter_ns()\n",
    "        self.read_data()\n",
    "        if function_timer: reading_time_stop = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: common_average_reference_time_start = time.perf_counter_ns()\n",
    "        self.common_average_reference()\n",
    "        if function_timer: common_average_reference_time_stop = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: filtering_time_start = time.perf_counter_ns()\n",
    "        self.filter_signal(eval=eval)\n",
    "        if function_timer: filtering_time_stop = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: formatting_time_start = time.perf_counter_ns()\n",
    "        self.format_data(window_size=window_size, duration_limit=duration_limit)\n",
    "        if function_timer: formatting_time_stop = time.perf_counter_ns()        \n",
    "        if function_timer: preprocessing_stop_time = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: print(f\"Reading Time: {(reading_time_stop - reading_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Common Average Reference Time: {(common_average_reference_time_stop - common_average_reference_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Filtering Time: {(filtering_time_stop - filtering_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Formatting Time: {(formatting_time_stop - formatting_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Total_time_spent_preprocessing: {(preprocessing_stop_time - preprocessing_start_time)//1e9}\")\n",
    "    \n",
    "        return self.X, self.y\n",
    "    \n",
    "    def process_portion(self, eval=False, window_size=20, duration_limit=900, function_timer=False,):\n",
    "        # Prereq: Data was manually read and is assigned after init of preprocessor with self.ecog_data & self.motion_data assignment\n",
    "        if function_timer: preprocessing_start_time = time.perf_counter_ns()\n",
    "        if function_timer: common_average_reference_time_start = time.perf_counter_ns()\n",
    "        self.common_average_reference()\n",
    "        if function_timer: common_average_reference_time_stop = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: filtering_time_start = time.perf_counter_ns()\n",
    "        self.filter_signal(eval=eval)\n",
    "        if function_timer: filtering_time_stop = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: formatting_time_start = time.perf_counter_ns()\n",
    "        self.format_data(window_size=window_size, duration_limit=duration_limit)\n",
    "        if function_timer: formatting_time_stop = time.perf_counter_ns()        \n",
    "        if function_timer: preprocessing_stop_time = time.perf_counter_ns()\n",
    "\n",
    "        if function_timer: print(f\"Common Average Reference Time: {(common_average_reference_time_stop - common_average_reference_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Filtering Time: {(filtering_time_stop - filtering_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Formatting Time: {(formatting_time_stop - formatting_time_start) / 1e9} seconds\")\n",
    "        if function_timer: print(f\"Total_time_spent_preprocessing: {(preprocessing_stop_time - preprocessing_start_time)//1e9}\")        \n",
    "        \n",
    "        return self.X, self.y\n",
    "    \n",
    "    # Testing Cleaning Data\n",
    "    # def clean_data(self,):\n",
    "        # # Read Data\n",
    "        # self.read_data()\n",
    "\n",
    "        # # Normalize with a common reference to improve model performance\n",
    "        # self.common_average_reference()\n",
    "\n",
    "        # # Filter Noise\n",
    "        # print(f\"Applying a bandpass filter from 1 KHz to 200 KHz\")\n",
    "        # ecog_raw = self.ecog_data[self.ecog_data.columns[1:-1]].values\n",
    "        # filtered = bandpass_filter(ecog_raw, lowcut=1.0, highcut=200.0, fs=1000.0, order=4)\n",
    "        # print(f\"Removing 60 Hz Electrical Noise with a Notch Filter\")\n",
    "        # denoised = notch_filter(filtered, freq=60, fs=1000.0)\n",
    "        # print(f\"Denoised Shape: {denoised.shape}\")\n",
    "\n",
    "        # # Trim the data to have a consistent 15 minute length\n",
    "        # DURATION_LIMIT = 900\n",
    "        # print(f\"Truncating the data to have the same 15 minute limit.\")\n",
    "        # ecog_df = self.ecog_data[self.ecog_data[\"Time\"] <= DURATION_LIMIT]\n",
    "        # motion_df = self.motion_data[self.motion_data[\"Motion_time\"] <= DURATION_LIMIT]\n",
    "\n",
    "        # ecog_values = ecog_df.drop(columns=[\"Fs\", \"Time\"]).values\n",
    "        # motion_values = motion_df.drop(columns=[\"Fsm\", \"Motion_time\"]).values\n",
    "\n",
    "        # print(f\"motion_values.shape: {motion_values.shape}\")\n",
    "        # print(f\"ecog_values.shape: {ecog_values.shape}\")\n",
    "\n",
    "        # # Replace in DataFrame\n",
    "        # self.ecog_data = self.ecog_data.copy()\n",
    "        # self.ecog_data[self.ecog_data.columns[1:-1]] = self.scaled_ecog\n",
    "\n",
    "        # # Clean memory\n",
    "        # del ecog_raw, filtered, denoised\n",
    "        # gc.collect()\n",
    "\n",
    "    def read_data(self):\n",
    "        print(\"Reading data\")\n",
    "        self.ecog_data = pd.read_csv(self.ecog_file_path)\n",
    "        self.motion_data = pd.read_csv(self.motion_file_path)\n",
    "        print(f\"self.ecog_data.shape:{self.ecog_data.shape}\")\n",
    "        print(f\"self.motion_data.shape:{self.motion_data.shape}\")\n",
    "        return self\n",
    "\n",
    "    def common_average_reference(self):\n",
    "        # Subtract the common mean from the signals \n",
    "        print(\"Subtracting common mean from the signals to create common average reference\")\n",
    "        print(f\"Before Common average reference: self.ecog_data.shape: {self.ecog_data.shape}\")\n",
    "        print(f\"Before Common average reference: self.motion_data.shape: {self.motion_data.shape}\")\n",
    "        common_average_reference = np.mean(self.ecog_data.drop([\"Time\", \"Fs\"], axis=1).values, axis=1, keepdims=1)\n",
    "        ecog_data_values = self.ecog_data[self.ecog_data.columns[1:-1]].values\n",
    "        ecog_data_common_mean_subtracted = ecog_data_values - common_average_reference\n",
    "        self.ecog_data[self.ecog_data.columns[1:-1]] = ecog_data_common_mean_subtracted\n",
    "        print(f\"After Common average reference: self.ecog_data.shape: {self.ecog_data.shape}\")\n",
    "        print(f\"After Common average reference: self.motion_data.shape: {self.motion_data.shape}\")\n",
    "        del ecog_data_values, ecog_data_common_mean_subtracted, common_average_reference                           \n",
    "        gc.collect()\n",
    "        return self\n",
    "\n",
    "    def filter_signal(self, eval=False):\n",
    "        ecog_raw = self.ecog_data[self.ecog_data.columns[1:-1]].values\n",
    "        print(f\"Raw Data Shape: {ecog_raw.shape}\")\n",
    "\n",
    "        # Apply filters\n",
    "        print(f\"Applying a bandpass filter from 1 KHz to 200 KHz\")\n",
    "        filtered = bandpass_filter(ecog_raw, lowcut=1.0, highcut=200.0, fs=1000.0, order=4)\n",
    "        print(f\"Removing 60 Hz Electrical Noise with a Notch Filter\")\n",
    "        denoised = notch_filter(filtered, freq=60, fs=1000.0)\n",
    "        print(f\"Denoised Shape: {denoised.shape}\")\n",
    "        # Evaluate filters\n",
    "        if eval:\n",
    "            kurt_raw = kurtosis(ecog_raw, axis=0, fisher=True)\n",
    "            kurt_denoised = kurtosis(denoised, axis=0, fisher=True)\n",
    "            proportion_of_positive_kurtosis_signals(kurt_raw, kurt_denoised)\n",
    "            compute_rmse(ecog_raw, denoised)\n",
    "\n",
    "        # Compute Power Envelopes\n",
    "        print(\"Computing Power Envelopes: Builds concatenated band-power features for μ, β, and high-gamma using a Hilbert transform\")\n",
    "        features = multiband_features(denoised, fs=1000.0) # shape (T, 192)\n",
    "        print(f\"Features Shape of the Multiband Features: {features.shape}\")\n",
    "\n",
    "        # Identify the principal components of the network\n",
    "        print(f\"Identifying 64 Principal components of the network\")\n",
    "        pca = PCA(n_components = 64, random_state=42)\n",
    "        reduced = pca.fit_transform(features)\n",
    "        print(f\"Reduced Shape from PCA: {reduced.shape}\")\n",
    "        # Scale\n",
    "        print(f\"Scaling the data to have a mean of 0 and standard deviation of 1\")\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_ecog = self.scaler.fit_transform(reduced)\n",
    "\n",
    "        # Replace in DataFrame\n",
    "        self.ecog_data = self.ecog_data.copy()\n",
    "        self.ecog_data[self.ecog_data.columns[1:-1]] = self.scaled_ecog\n",
    "\n",
    "        # Clean memory\n",
    "        del ecog_raw, filtered, denoised\n",
    "        gc.collect()\n",
    "        return self\n",
    "\n",
    "    def format_data(self, window_size=20, duration_limit=900):\n",
    "        print(f\"This data has been preprocessed.\")\n",
    "        print(f\"Truncating the data to have the same 15 minute limit.\")\n",
    "        ecog_df = self.ecog_data[self.ecog_data[\"Time\"] <= duration_limit]\n",
    "        motion_df = self.motion_data[self.motion_data[\"Motion_time\"] <= duration_limit]\n",
    "\n",
    "        ecog_values = ecog_df.drop(columns=[\"Fs\", \"Time\"]).values\n",
    "        motion_values = motion_df.drop(columns=[\"Fsm\", \"Motion_time\"]).values\n",
    "\n",
    "        print(f\"motion_values.shape: {motion_values.shape}\")\n",
    "        print(f\"ecog_values.shape: {ecog_values.shape}\")\n",
    "\n",
    "        # Smooth the signal\n",
    "        print(f\"Creating Overlapping Windows of data to Smooth the Signal\")\n",
    "        X, y = create_overlapping_windows(ecog_values, motion_values, window_size=20, hop_size=10)\n",
    "        print(f\"y.shape: {y.shape}\")\n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "        print(self.X.shape)\n",
    "        print(self.y.shape)\n",
    "        \n",
    "        # Clean up\n",
    "        del ecog_values, motion_values\n",
    "        gc.collect()\n",
    "\n",
    "    def save(self):\n",
    "        output_file_path_base = self.ecog_file_path.strip(\"ecog_data.csv\")\n",
    "        joblib.dump(self.scaler, output_file_path_base + \"scaler_ecog.pkl\")\n",
    "        np.save(output_file_path_base + \"X.npy\", self.X)\n",
    "        np.save(output_file_path_base + \"y.npy\", self.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347cade",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f649564",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRALATERAL_BASE_PATH =  '../data/'\n",
    "ECOG_DATA_FILENAME = 'Contralateral_2018-04-12_(S4)_cleaned_aligned_ecog_data.csv'\n",
    "ECOG_DATA_FILENAME_DATA_ONLY = 'Contralateral_2018-04-12_(S4)_cleaned_aligned_ecog_data_DATA_ONLY.csv'\n",
    "MOTION_DATA_FILENAME = 'Contralateral_2018-04-12_(S4)_cleaned_aligned_motion_data.csv'\n",
    "MOTION_DATA_FILENAME_DATA_ONLY = 'Contralateral_2018-04-12_(S4)_cleaned_aligned_motion_data_DATA_ONLY.csv'\n",
    "CONTRALATERAL_ECOG_DATA_FULL_FILE_PATH = CONTRALATERAL_BASE_PATH + ECOG_DATA_FILENAME\n",
    "CONTRALATERAL_MOTION_DATA_FULL_FILE_PATH = CONTRALATERAL_BASE_PATH + MOTION_DATA_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af3bb5",
   "metadata": {},
   "source": [
    "## Training the model from Raw Data\n",
    "\n",
    "- Raw Ecog Data is of shape (900000, 64) for 900 seconds @ 1000 Hz for 64 Channels.\n",
    "- Raw Motion Data is of shape (45000, 3) for 900 seconds @ 50 Hz for 3 coordinates: x, y, z.\n",
    "- The processed Ecog Data is transformed into shape (89999, 20, 64) for 89999 samples of 20 batches of 64 principal components of 3 power spectral frequency bands smoothed with a 20 ms window and 50% overlap.\n",
    "- The motion data is transformed to shape 89999, 3 for 89999 samples of 3 positions where the data is smoothed with a 20ms window and 50% overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "# self.read_data()\n",
    "# ecog_file_path = \n",
    "# motion_file_path = \n",
    "# Raw data Collection\n",
    "motion_data_file_l = glob(os.path.join(os.getcwd(), \"src/\", \"motor_coartex/data/data/\", \"**\", \"motion*.csv\"), recursive=True)\n",
    "ecog_data_file_l = glob(os.path.join(os.getcwd(), \"src/\", \"motor_cortex/data/data/\", \"**\", \"ecog*.csv\"), recursive=True)\n",
    "\n",
    "# SESSION_SET: '/home/linux-pc/gh/CRCNS/src/motor_cortex/data/data/Ipsilateral/2018-05-03_(S3)/X.npy', 4 # This is known from evaluating on several training sets\n",
    "# best_epoch: best_epoch: 38 | best_val_loss: 0.6026811446545653\n",
    "# Index of best Session_Set: 22\n",
    "INDEX = 22\n",
    "current_ecog_data_file = ecog_data_file_l[INDEX]\n",
    "current_motion_data_file = motion_data_file_l[INDEX]\n",
    "\n",
    "print(f\"current_ecog_data_file:{current_ecog_data_file}\")\n",
    "print(f\"current_motion_data_file:{current_motion_data_file}\")\n",
    "\n",
    "ecog_file_path = current_ecog_data_file\n",
    "motion_file_path = current_motion_data_file\n",
    "\n",
    "ecog_data = pd.read_csv(ecog_file_path)\n",
    "motion_data = pd.read_csv(motion_file_path)\n",
    "print(f\"self.ecog_data.shape:{ecog_data.shape}\")\n",
    "print(f\"self.motion_data.shape:{motion_data.shape}\")\n",
    "\n",
    "# Normalize with a common reference to improve model performance\n",
    "# self.common_average_reference()\n",
    "# Subtract the common mean from the signals \n",
    "\n",
    "print(\"Subtracting common mean from the signals to create common average reference\")\n",
    "print(f\"Before Common average reference: self.ecog_data.shape: {ecog_data.shape}\")\n",
    "print(f\"Before Common average reference: self.motion_data.shape: {motion_data.shape}\")\n",
    "common_average_reference = np.mean(ecog_data.drop([\"Time\", \"Fs\"], axis=1).values, axis=1, keepdims=1)\n",
    "\n",
    "ecog_data_values = ecog_data[ecog_data.columns[1:-1]].values\n",
    "ecog_data_common_mean_subtracted = ecog_data_values - common_average_reference\n",
    "ecog_data[ecog_data.columns[1:-1]] = ecog_data_common_mean_subtracted\n",
    "\n",
    "print(f\"After Common average reference: self.ecog_data.shape: {ecog_data.shape}\")\n",
    "print(f\"After Common average reference: self.motion_data.shape: {motion_data.shape}\")\n",
    "del ecog_data_values, ecog_data_common_mean_subtracted, common_average_reference                           \n",
    "gc.collect()\n",
    "\n",
    "# Filter Noise\n",
    "print(f\"Applying a bandpass filter from 1 KHz to 200 KHz\")\n",
    "ecog_raw = ecog_data[ecog_data.columns[1:-1]].values\n",
    "filtered = bandpass_filter(ecog_raw, lowcut=1.0, highcut=200.0, fs=1000.0, order=4)\n",
    "print(f\"Removing 60 Hz Electrical Noise with a Notch Filter\")\n",
    "denoised = notch_filter(filtered, freq=60, fs=1000.0)\n",
    "print(f\"Denoised Shape: {denoised.shape}\")\n",
    "print(f\"motion data shape: {motion_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Denoised Shape: {denoised.shape}\")\n",
    "print(f\"motion data shape: {motion_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION_LIMIT = 900\n",
    "print(f\"Truncating the data to have the same 15 minute limit.\")\n",
    "ecog_data_denoised = ecog_data.copy()\n",
    "ecog_data_denoised[ecog_data_denoised.columns[1:-1]] = denoised\n",
    "\n",
    "ecog_df = ecog_data_denoised[ecog_data_denoised[\"Time\"] <= DURATION_LIMIT]\n",
    "motion_df = motion_data[motion_data[\"Motion_time\"] <= DURATION_LIMIT]\n",
    "ecog_values = ecog_df.drop(columns=[\"Fs\", \"Time\"]).values\n",
    "motion_values = motion_df.drop(columns=[\"Fsm\", \"Motion_time\"]).values\n",
    "print(f\"motion_values.shape: {motion_values.shape}\")\n",
    "print(f\"ecog_values.shape: {ecog_values.shape}\")\n",
    "\n",
    "# Clean memory\n",
    "del ecog_raw, filtered, denoised, ecog_data, ecog_data_denoised\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50beda88",
   "metadata": {},
   "source": [
    "## Loading Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_df = pd.read_csv(CONTRALATERAL_ECOG_DATA_FULL_FILE_PATH)\n",
    "motion_df = pd.read_csv(CONTRALATERAL_MOTION_DATA_FULL_FILE_PATH)\n",
    "\n",
    "print(f\"motion_df.shape: {motion_df.shape}\")\n",
    "print(f\"ecog_df.shape: {ecog_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db763fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_df.drop(columns=[\"Time\", \"Fs\"]).to_csv(CONTRALATERAL_BASE_PATH + ECOG_DATA_FILENAME_DATA_ONLY)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70395b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df.drop(columns=[\"Fsm\", \"Motion_time\"]).to_csv(CONTRALATERAL_BASE_PATH + MOTION_DATA_FILENAME_DATA_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89da16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad7112",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=motion_df['Left_Wrist_X'],\n",
    "    y=motion_df['Left_Wrist_Y'],\n",
    "    z=motion_df['Left_Wrist_Z'],\n",
    "    mode='lines',\n",
    "    line=dict(color='teal', width=3),\n",
    "    name='Trajectory'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Interactive 3D Wrist Motion\",\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z',\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93750547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Use smaller subset for performance (e.g. every 10th sample)\n",
    "step = 10\n",
    "x_vals = motion_df['Left_Wrist_X'].values[::step]\n",
    "y_vals = motion_df['Left_Wrist_Y'].values[::step]\n",
    "z_vals = motion_df['Left_Wrist_Z'].values[::step]\n",
    "\n",
    "# Create frames for animation\n",
    "frames = [\n",
    "    go.Frame(\n",
    "        data=[go.Scatter3d(\n",
    "            x=x_vals[:k],\n",
    "            y=y_vals[:k],\n",
    "            z=z_vals[:k],\n",
    "            mode='lines+markers',\n",
    "            line=dict(color='teal', width=4),\n",
    "            marker=dict(size=3, color='purple')\n",
    "        )],\n",
    "        name=str(k)\n",
    "    )\n",
    "    for k in range(1, len(x_vals) + 1)\n",
    "]\n",
    "\n",
    "# Create initial trace\n",
    "initial_trace = go.Scatter3d(\n",
    "    x=[x_vals[0]],\n",
    "    y=[y_vals[0]],\n",
    "    z=[z_vals[0]],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='purple')\n",
    ")\n",
    "\n",
    "# Build the figure\n",
    "fig = go.Figure(\n",
    "    data=[initial_trace],\n",
    "    frames=frames\n",
    ")\n",
    "\n",
    "# Layout with play/pause buttons and slider\n",
    "fig.update_layout(\n",
    "    title='Animated 3D Wrist Motion',\n",
    "    scene=dict(\n",
    "        xaxis=dict(range=[x_vals.min(), x_vals.max()], title='X'),\n",
    "        yaxis=dict(range=[y_vals.min(), y_vals.max()], title='Y'),\n",
    "        zaxis=dict(range=[z_vals.min(), z_vals.max()], title='Z'),\n",
    "    ),\n",
    "    updatemenus=[dict(\n",
    "        type='buttons',\n",
    "        showactive=False,\n",
    "        buttons=[\n",
    "            dict(\n",
    "                label='Play',\n",
    "                method='animate',\n",
    "                args=[\n",
    "                    None,\n",
    "                    dict(frame=dict(duration=20, redraw=True),\n",
    "                         fromcurrent=True, mode='immediate')\n",
    "                ]\n",
    "            ),\n",
    "            dict(\n",
    "                label='Pause',\n",
    "                method='animate',\n",
    "                args=[\n",
    "                    [None],\n",
    "                    dict(frame=dict(duration=0, redraw=False),\n",
    "                         mode='immediate')\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        x=0.1,\n",
    "        y=0,\n",
    "        xanchor='right',\n",
    "        yanchor='top'\n",
    "    )],\n",
    "    sliders=[dict(\n",
    "        active=0,\n",
    "        pad=dict(t=50),\n",
    "        steps=[\n",
    "            dict(\n",
    "                method='animate',\n",
    "                args=[[str(k)], dict(mode='immediate', frame=dict(duration=0, redraw=True), transition=dict(duration=0))],\n",
    "                label=str(k)\n",
    "            )\n",
    "            for k in range(1, len(x_vals) + 1, 20)\n",
    "        ]\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
